{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2: Leveraging Foundation Models for Robust Supervised Learning\n",
        "\n",
        "In last week's project, you saw how important choosing the right features is to creating a successful ML model. For more complex modalities like language, speech, and vision, we didn't use the best features we could have! In the week 1 reference notebook, we used standard baseline feature encodings for these modalities. In this project we will use *foundation model features*. These models can be trained on large, related datasets and from them we can extract general features for prediction tasks.\n",
        "\n",
        "In this project, we will introduce several Kaggle challenge datasets for further improvement via foundation models! The project notebook will introduce a few different foundation models to use as features, and establish a basic baseline system with each. You can leverage these models along with anything else you explored in Week 1 with the continued goal of using build-measure-learn iterations to achieve the best system you can.\n",
        "\n",
        "Foundation models are often complex deep learning models with large datasets and many parameters. We provide pre-trained foundation models for use in this project. If you want to learn more about designing and training foundation models, check out the Uplimit introduction to deep learning course!\n",
        "\n",
        "Note: foundation models apply to modalities like language, speech, and vision, but they generally aren't used for featurizing tabular data. If you would like to continue working on tabular data, you are welcome to work on Dataset 1 (the transaction fraud dataset). Otherwise, you can try foundation models on one of the other datasets.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. We provide starter code below as a scaffold. You will be using many of the skills you learned from previous weeks to complete this notebook.\n",
        "2. Ensure you read through the document and starting code before beginning your work. Understand the overall structure and goals of the project to make your iteration smoother.\n",
        "3. As in Week 1, keep track of what you try and iterate towards building the best ML system you can! You are also welcome to try some targeted evaluations of your model to see if e.g. foundation features make the model more robust to noisy or transformed inputs."
      ],
      "metadata": {
        "id": "Rbv7MqCl5HQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies\n",
        "\n",
        "We first setup the libraries required for the project. Many of these may already be installed by default in Colab."
      ],
      "metadata": {
        "id": "5pKOzjAeh1Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install librosa\n",
        "!pip install xgboost\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "F33cOjFVh3I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# importing a potpouri of models you can use\n",
        "# feel free to add more!\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "OoDx5jGkiIqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shared setup code for datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class BaseDataset:\n",
        "  \"\"\"\n",
        "  We will use this base class for all datasets.\n",
        "  You do not need to change this class.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self._data = self.make_data()\n",
        "\n",
        "  def _load(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def make_data(self):\n",
        "    print('loading data...')\n",
        "    X_train, y_train = self._load()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
        "    print('done.')\n",
        "    return dict(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test)\n",
        "\n",
        "  def get_train_data(self):\n",
        "    return self._data['X_train'], self._data['y_train']\n",
        "\n",
        "  def get_test_data(self):\n",
        "    return self._data['X_test'], self._data['y_test']\n",
        "\n",
        "  @property\n",
        "  def num_train(self):\n",
        "    return len(self._data['X_train'])\n",
        "\n",
        "  @property\n",
        "  def num_test(self):\n",
        "    return len(self._data['X_test'])\n"
      ],
      "metadata": {
        "id": "CC6fxKmicbEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle challenge datasets"
      ],
      "metadata": {
        "id": "daq-vG0dyzEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chosen a few datasets from Kaggle as possible tasks for you. Each of these datasets was chosen so that we can leverage foundation features for different input modalities. Your project this week is to choose a dataset, and build the best performing model you can by leveraging modeling iterations and foundation features."
      ],
      "metadata": {
        "id": "QGsWQhog63Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Task: Choose ONE dataset. Use foundation features to achieve the best performance you can**\n",
        "Choose a dataset and leverage foundation features along with your modeling best practices to achieve good performance.\n",
        "\n",
        "We have pre-computed foundation features for you using different models applicable to each task domain. Before starting work you might want to briefly review each of the datasets and corresponding foundation models.\n",
        "\n",
        "Report your best performance. For practice, ensure you can summarize what your final model is, and what you tried along the way. We provide a _research notebook_ starting point at the bottom for you to track your work.\n",
        "\n",
        "_You only need to work on one of the datasets below, but try more than one if you'd like!_"
      ],
      "metadata": {
        "id": "CdZNwNvj7GTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# useful general functions\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_svm(X_train, y_train):\n",
        "  model = LinearSVC()\n",
        "  model.fit(X_train, y_train)\n",
        "  return model\n",
        "\n",
        "def predict_svm(model, X):\n",
        "  return model.predict(X)"
      ],
      "metadata": {
        "id": "DwFefqkDphQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 1: Transaction Fraud Detection\n",
        "\n",
        "[Kaggle link](https://www.kaggle.com/c/ieee-fraud-detection/overview)\n",
        "\n",
        "This dataset contains Vesta's real world e-commerce transactions with features from device type to product types. The challenge is to design a model to classify fraudulent transactions, helping businesses reduce loss.\n",
        "\n",
        "**Transaction Features:**\n",
        "\n",
        "- `TransactionDT`: timedelta from a given reference datetime (not an actual timestamp)\n",
        "- `TransactionAMT`: transaction payment amount in USD\n",
        "- `ProductCD`: product code, the product for each transaction\n",
        "- `card1` - `card6`: payment card information, such as card type, card category, issue bank, country, etc.\n",
        "- `addr`: address\n",
        "- `dist`: distance\n",
        "- `P_` and (`R__`) `emaildomain`: purchaser and recipient email domain\n",
        "- `C1`-`C14`: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
        "- `D1`-`D15`: timedelta, such as days between previous transaction, etc.\n",
        "- `M1`-`M9`: match, such as names on card and address, etc.\n",
        "- `Vxxx`: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
        "\n",
        "**Identity Features:**\n",
        "\n",
        "Variables in this table are identity information â€“ network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n",
        "They're collected by Vestaâ€™s fraud protection system and digital security partners.\n",
        "(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n",
        "\n",
        "\n",
        "The following are categorical features:\n",
        "`ProductCD`, `card1` - `card6`, `addr1`, `addr2`, `P_emaildomain`, `R_emaildomain`, `M1` - `M9`, `DeviceType`, `DeviceInfo`, `id_12` - `id_38`. We recommend you handle categorical features by converting them to [one-hot representations](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/).\n",
        "\n",
        "Further, this dataset may have missing entries, as is common in tabular data. You have many options here: you can drop rows with missing data, or replace with a filler value, or try to impute it with similar values. It is up to you!"
      ],
      "metadata": {
        "id": "p2IqG9uf0pPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=11_y7TCGE3YRL_qW33XVVWUILlrrkzcSZ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=11_y7TCGE3YRL_qW33XVVWUILlrrkzcSZ\" -O train_transaction.csv && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1c1u1zKKVz6FnbcMUM6yUzrigqfK6bQn2' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1c1u1zKKVz6FnbcMUM6yUzrigqfK6bQn2\" -O train_identity.csv && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "SU63dPOf1Bkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class FraudDataset(BaseDataset):\n",
        "\n",
        "  def _load(self):\n",
        "    rs = np.random.RandomState(42)\n",
        "\n",
        "    train_tx = pd.read_csv('./train_transaction.csv')\n",
        "    train_id = pd.read_csv('./train_identity.csv')\n",
        "    train_data = train_tx.merge(train_id, on='TransactionID', how='left')\n",
        "    train_data.reset_index(inplace=True)\n",
        "    del train_data['TransactionID']\n",
        "    train_label = train_data['isFraud']\n",
        "    del train_data['isFraud']\n",
        "\n",
        "    # subsample 10k positive and negative!\n",
        "    indices0 = rs.choice(np.where(train_label == 0)[0], 10000, replace=False)\n",
        "    indices1 = rs.choice(np.where(train_label == 1)[0], 10000, replace=False)\n",
        "    indices = np.concatenate([indices0, indices1])\n",
        "    train_data = train_data.iloc[indices]\n",
        "    train_label = train_label.iloc[indices]\n",
        "\n",
        "    return train_data, train_label\n",
        "\n",
        "\n",
        "dataset = FraudDataset()"
      ],
      "metadata": {
        "id": "sCM099yu5v4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = dataset.get_train_data()\n",
        "X_test, y_test = dataset.get_test_data()\n",
        "print('Raw Input:')\n",
        "print(X_train.head())\n",
        "print('Targets:')\n",
        "print(y_train.head())"
      ],
      "metadata": {
        "id": "4B12kLyT6OSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterating on Tabular Datasets\n",
        "\n",
        "Foundation models apply to modalities like language, speech, and vision, but they generally aren't used for featurizing tabular data. If you would like to continue working with tabular data and choose to iterate on this fraud dataset, there are more steps you can take to improve model performance:\n",
        "\n",
        "1.   Try scikit-learn's [built-in preprocessing methods](https://scikit-learn.org/stable/modules/preprocessing.html) for standardizing continuous features and encoding categorical features.\n",
        "2.   Try different methods for imputing missing values. Scikit-learn provides many classes like [`SimpleImputer`, `IterativeImputer`, and `KNNImputer`](https://scikit-learn.org/stable/modules/impute.html) that you can try out.\n",
        "3. Try some of the [Tier-2 advanced models](https://colab.research.google.com/drive/1nx7V_XHrWc5RNJ187aGT18gHwp5JTC1h#scrollTo=PzaSS4S7veeZ) suggested in last week's SOTA reference notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "JdeAnHyG6ntz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Previous learners have been able to achieve >80% test accuracy on the transaction fraud dataset, depending on their choices of model + features.*"
      ],
      "metadata": {
        "id": "LoiSTBd-K631"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 2: Disaster Prediction from Tweets"
      ],
      "metadata": {
        "id": "KybFCKyzqnZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[(kaggle link)](https://www.kaggle.com/c/nlp-getting-started/overview)\n",
        "\n",
        "Tweets are an important communication channel in times of emergency. Ideally, our protection agencies can programmatically monitor Twitter to detect disasters and provide relief. However, Tweets that may sound that it is reporting a disaster may be referring to something else entirely. This dataset contains a collection of tweet texts annotated with binary labels that indicate whether the tweet describes a real disaster or not. Additional features, such as location and keyword may be provided."
      ],
      "metadata": {
        "id": "t_Di2c1g1Pze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1NfuR0tuBF0t5HJW2Q12l0c0hnAH1VqTj"
      ],
      "metadata": {
        "id": "Bs8dWVffqdYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "class TweetDataset(BaseDataset):\n",
        "\n",
        "  def _load(self):\n",
        "    train_data = pd.read_csv('./train.csv')\n",
        "    train_label = train_data['target']\n",
        "    del train_data['id'], train_data['target']\n",
        "    return train_data, train_label\n",
        "\n",
        "# --\n",
        "dataset = TweetDataset()"
      ],
      "metadata": {
        "id": "lRycDcEtqeMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = dataset.get_train_data()\n",
        "X_test, y_test = dataset.get_test_data()\n",
        "print(f'Train shape: {X_train.shape}')\n",
        "print(f'Test shape: {X_test.shape}')"
      ],
      "metadata": {
        "id": "C32gb8aoqlT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "0LqU869N1X8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[(blog)](https://jalammar.github.io/illustrated-word2vec/) [(paper)](https://arxiv.org/pdf/1310.4546.pdf)\n",
        "\n",
        "Word2Vec is a very popular algorithm to map individual words to high dimensional vector representations. It does so in a way that synonymous words will be close to each other in vector space. In fact, a famous example for Word2Vec is that embedding of `king` - embedding of `man` + embedding of `woman` returns the embedding of `queen`. This example illustrates that these representations hold semantic meaning.\n",
        "\n",
        "Word2Vec is trained on a large text corpus, from Twitter to a collection of books. The training distribution has a large effect on the representations learned! The advantage of Word2Vec is simplicity: the model is stored as a large dictionary from words to embeddings. Additionally, it captures more complex behavior than TF-IDF (which we saw from week 1). The downside is still its assumption on word independence. We know as language speakers that words are rarely used in isolation. As such, the perfect representation should capture the context a word is used in; Word2Vec sacrifices this. In practice, Word2Vec returns an embedding for every word in the sentence but we average across all words in a sentence to achieve a single embedding per tweet."
      ],
      "metadata": {
        "id": "wwF2vsEoLrgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# precomputed word2vec embeddings on the TweetDataset\n",
        "!gdown --id 11AY0dD_FZ4ghoIr-Jt8TyledvXvTTdTF\n",
        "!gdown --id 1MFByPWEuqFjKXje-TjX90JPlbmoAuHJq"
      ],
      "metadata": {
        "id": "ahy6xWiroKZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# these will be in the same order and size as X_train/y_train from above\n",
        "X_word2vec_train = np.load('./tweet_word2vec_train.npy', allow_pickle=True)\n",
        "X_word2vec_test = np.load('./tweet_word2vec_test.npy', allow_pickle=True)\n",
        "print(f'Train shape: {X_word2vec_train.shape}')\n",
        "print(f'Test shape: {X_word2vec_test.shape}')"
      ],
      "metadata": {
        "id": "kQURtsb-oSzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below establishes a baseline model training and evaluation experiment using word2vec features. You may use this as a comparison point for performance, and a start improving from here."
      ],
      "metadata": {
        "id": "zVyxfv23YHGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_svm(X_word2vec_train, y_train)\n",
        "y_hat_train = predict_svm(model, X_word2vec_train)\n",
        "y_hat_test = predict_svm(model, X_word2vec_test)\n",
        "train_acc = accuracy_score(y_train, y_hat_train)\n",
        "test_acc = accuracy_score(y_test, y_hat_test)\n",
        "print(f'Train accuracy: {train_acc}')\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "DVLpc18_pm_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "8St2A9eb1cqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[(blog)](https://jalammar.github.io/illustrated-bert/) [(paper)](https://arxiv.org/abs/1810.04805)\n",
        "\n",
        "In 2018, BERT shocked the NLP research world as it crushed its competition on a variety of benchmarks. BERT learned **contextual** word embeddings by mixing features from individual tokens using the popular Transformer network. Unlike Word2Vec, BERT embeddings captured the greater context of the sentence and document that a word was being used in. We linked an amazing blog above that we highly recommend the curious reader to explore.\n",
        "\n",
        "Below, we precomputed embeddings for two different kinds of BERT, one trained on a large corpus of internet articles and books, and the other trained on a large corpus of Tweets. You are free to experiment with both, or combine them!\n"
      ],
      "metadata": {
        "id": "8TH47qSZLy-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download BERT\n",
        "!gdown --id 18ryiowk_A73UyTB8Mw3zhqbclbPPaGDc\n",
        "!gdown --id 1u0XENXcs8A_D96EHIN2iENMm6qR7IyAJ"
      ],
      "metadata": {
        "id": "gLDnPPAU6hf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bert_train = np.load('./tweet_bert_train.npy')\n",
        "X_bert_test = np.load('./tweet_bert_test.npy')\n",
        "print(f'Train shape: {X_bert_train.shape}')\n",
        "print(f'Test shape: {X_bert_test.shape}')"
      ],
      "metadata": {
        "id": "r86ANnpu_m_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a baseline setup to use BERT features with an SVM classifier."
      ],
      "metadata": {
        "id": "acZHf7ouYYfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_svm(X_bert_train, y_train)\n",
        "y_hat_train = predict_svm(model, X_bert_train)\n",
        "y_hat_test = predict_svm(model, X_bert_test)\n",
        "train_acc = accuracy_score(y_train, y_hat_train)\n",
        "test_acc = accuracy_score(y_test, y_hat_test)\n",
        "print(f'Train accuracy: {train_acc}')\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "dNfujqMGqBLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download BERTweet\n",
        "!gdown --id 1-Ef8QkuhYVClgIra-AWFmMV0L0K7zWJ7\n",
        "!gdown --id 11gURmayDn1TsMGYY7guelJ9Khs_bnFdC"
      ],
      "metadata": {
        "id": "B0SgYSb5_Zjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bertweet_train = np.load('./tweet_bertweet_train.npy')\n",
        "X_bertweet_test = np.load('./tweet_bertweet_test.npy')\n",
        "print(f'Train shape: {X_bertweet_train.shape}')\n",
        "print(f'Test shape: {X_bertweet_test.shape}')"
      ],
      "metadata": {
        "id": "y5VL-N8__403"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below establishes a baseline model using the tweet-specific BERT features. You may use this as a baseline, and combine the features in whatever way you choose to achieve the best final system performance you can below."
      ],
      "metadata": {
        "id": "HVtqS1CbYmFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_svm(X_bertweet_train, y_train)\n",
        "y_hat_train = predict_svm(model, X_bertweet_train)\n",
        "y_hat_test = predict_svm(model, X_bertweet_test)\n",
        "train_acc = accuracy_score(y_train, y_hat_train)\n",
        "test_acc = accuracy_score(y_test, y_hat_test)\n",
        "print(f'Train accuracy: {train_acc}')\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "iJltWM-uqJHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Previous learners have been able to achieve ~83% test accuracy on the Tweet dataset. Try comparing classical NLP methods like TF-IDF with foundation model features like Word2Vec and BERT.*"
      ],
      "metadata": {
        "id": "k2Q2Eoy4JQwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 3: Classifying Cats and Dogs"
      ],
      "metadata": {
        "id": "Rf9-ex0tW898"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[(kaggle link)](https://www.kaggle.com/c/dogs-vs-cats)\n",
        "\n",
        "Is this an image of a cat or a dog? This training dataset contains 25,000 images of both animals. These are real world images of pets with different camera angles, backgrounds, and quality. In other words, this is a difficult task! The top performing model scores 98.9% but use more sophisticated methods than shown in this notebook. Still, see how well you can do!"
      ],
      "metadata": {
        "id": "Y__hVZ7AL9ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1ya_pBnNQ72Rw9AG0-6sZNRnt2ds_mBfP\n",
        "!unzip -q train.zip"
      ],
      "metadata": {
        "id": "oHiyT6aGAO6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "class CatDogDataset(BaseDataset):\n",
        "\n",
        "  def _load(self):\n",
        "    cat_files = glob('train/cat.*.jpg')\n",
        "    dog_files = glob('train/dog.*.jpg')\n",
        "    img_files = cat_files + dog_files\n",
        "    labels = [0] * len(cat_files) + [1] * len(dog_files)\n",
        "    data = np.array(img_files)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# --\n",
        "dataset = CatDogDataset()"
      ],
      "metadata": {
        "id": "dkBQ5GryAVxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = dataset.get_train_data()\n",
        "X_test, y_test = dataset.get_test_data()\n",
        "print(f'Train shape: {X_train.shape}')\n",
        "print(f'Test shape: {X_test.shape}')"
      ],
      "metadata": {
        "id": "bL9EEbFeAyYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet50\n",
        "\n",
        "[(blog)](https://towardsdatascience.com/introduction-to-resnets-c0a830a288a4) [(paper)](https://arxiv.org/abs/1512.03385)\n",
        "\n",
        "Residual Networks were one of the first neural networks to have a truly deep architecture e.g. 50 layers. They accomplished this through \"residual connections\" where the output of an early layer is directly fed as input to a layer further down the network. ResNets, being trained on ImageNet -- the benchmark for a large visual dataset -- are widely used to generate features for general images."
      ],
      "metadata": {
        "id": "GbC-cTfNMBir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1zcgfCH_bJiFn09ulR_ILbYF1OtM3UWt7\n",
        "!gdown --id 1wjzFlhwzYXeRONBVxs6r8sIS8zslvRsC"
      ],
      "metadata": {
        "id": "9tG3Iv8EBzGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_resnet50_train = np.load('./catdog_resnet50_train.npy')\n",
        "X_resnet50_test = np.load('./catdog_resnet50_test.npy')\n",
        "print(f'Train shape: {X_resnet50_train.shape}')\n",
        "print(f'Test shape: {X_resnet50_test.shape}')"
      ],
      "metadata": {
        "id": "KdwPucJ8F38i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below establishes a baseline model training and evaluation experiment using the ResNet features. You may use this as a comparison point for performance, and a starting point for integrating these features into ideas or other approaches you explored in Week 1 for this dataset."
      ],
      "metadata": {
        "id": "Uph5L_I3Tuyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_svm(X_resnet50_train, y_train)\n",
        "y_hat_train = predict_svm(model, X_resnet50_train)\n",
        "y_hat_test = predict_svm(model, X_resnet50_test)\n",
        "train_acc = accuracy_score(y_train, y_hat_train)\n",
        "test_acc = accuracy_score(y_test, y_hat_test)\n",
        "print(f'Train accuracy: {train_acc}')\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "A3P8FJ3Gqo2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP\n",
        "\n",
        "[(blog)](https://openai.com/blog/clip/) [(paper)](https://arxiv.org/abs/2103.00020)\n",
        "\n",
        "CLIP (or Visual Transformers)  is an unsupervised neural network released by OpenAI in 2021. It is trained to jointly learn image and text embeddings through a large corpus build from scraping the web. At the time of publication, CLIP is state of the art, and is becoming increasingly popular as a model for image (and text) features."
      ],
      "metadata": {
        "id": "wGepsmWPMFuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1RinM0zUNUllTUXL6yUjcvKQFExGyUGJP\n",
        "!gdown --id 1YgypozUZqzOjUJOqRPltgcW5KCAI0UeD"
      ],
      "metadata": {
        "id": "S85K9rfIEXYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_clip_train = np.load('./catdog_clip_train.npy')\n",
        "X_clip_test = np.load('./catdog_clip_test.npy')\n",
        "print(f'Train shape: {X_clip_train.shape}')\n",
        "print(f'Test shape: {X_clip_test.shape}')"
      ],
      "metadata": {
        "id": "WeKATt9zFlRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below establishes a baseline model training and evaluation experiment using CLIP features. You may use this as a comparison point for performance, and a starting point for further work with these features."
      ],
      "metadata": {
        "id": "JaaQR_4nWVo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_svm(X_clip_train, y_train)\n",
        "y_hat_train = predict_svm(model, X_clip_train)\n",
        "y_hat_test = predict_svm(model, X_clip_test)\n",
        "train_acc = accuracy_score(y_train, y_hat_train)\n",
        "test_acc = accuracy_score(y_test, y_hat_test)\n",
        "print(f'Train accuracy: {train_acc}')\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "pTvV-TAdqvY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*CLIP features achieve close to 100% accuracy. How does this compare with a classical image featurization method like HOG?*"
      ],
      "metadata": {
        "id": "y9gMEPUCLMjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 4: Google Home Command Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "__E-vfyrWp8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "[(kaggle link)](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview)\n",
        "\n",
        "\n",
        "Google Home, and similar smart devices, rely on speech models to detect when the user utters commands, like \"Hey Google\". This dataset contains 65,000 one-second long utterances of 30 different short words, each uttered by thousands of people. The labels you will need to predict are `yes`, `no`, `up`, `down`, `left`, `right`, `on`, `off`, `stop`, `go`. You should ignore all other classes."
      ],
      "metadata": {
        "id": "19S_y9jldJrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1sfkLsKT8JHPMM1pifQJqefL5elopjFX7\n",
        "!7z x train.7z -y"
      ],
      "metadata": {
        "id": "Cfr_TrJIdL3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "class CommandDataset(BaseDataset):\n",
        "  _commands = ['yes', 'no', 'up', 'down', 'left', 'right',\n",
        "               'on', 'off', 'stop', 'go']\n",
        "  _sample_rate = 16000\n",
        "\n",
        "  def _load(self):\n",
        "    data, labels = [], []\n",
        "    for c, command in enumerate(self._commands):\n",
        "      files = glob(os.path.join(f'./train/audio/{command}/*.wav'))\n",
        "      labels_c = [c] * len(files)\n",
        "      data += files\n",
        "      labels += labels_c\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# --\n",
        "dataset = CommandDataset()"
      ],
      "metadata": {
        "id": "fKXIKaspGiDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = dataset.get_train_data()\n",
        "X_test, y_test = dataset.get_test_data()\n",
        "print(f'Train shape: {X_train.shape}')\n",
        "print(f'Test shape: {X_test.shape}')"
      ],
      "metadata": {
        "id": "xo6W21RSILAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's revisit the two foundation models we used above, and see how they compare on this new dataset."
      ],
      "metadata": {
        "id": "ZSs3lNHtHkDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wav2vec 2.0"
      ],
      "metadata": {
        "id": "HiRi3jEG53Fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[(blog)](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) [(paper)](https://arxiv.org/abs/2006.11477)"
      ],
      "metadata": {
        "id": "VxPYdSm5GwYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1_QKzTYqR28K3LCaFBWGIRqKx1ieFPSXj\n",
        "!gdown --id 17Tv7Tk_uOtbwmIRb_h9sPXcyMCpWkKTa"
      ],
      "metadata": {
        "id": "bnWH2hJqodhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_wav2vec_train = np.load('./google_wav2vec_train.npy')\n",
        "X_wav2vec_test = np.load('./google_wav2vec_test.npy')\n",
        "print(f'Train shape: {X_wav2vec_train.shape}')\n",
        "print(f'Test shape: {X_wav2vec_test.shape}')"
      ],
      "metadata": {
        "id": "Pw3EDdUhpM-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the foundation feature reference notebook, this establishes a baseline classifier using the"
      ],
      "metadata": {
        "id": "-b0U5XzDZH66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_svm(X_wav2vec_train, y_train)\n",
        "y_hat_train = predict_svm(model, X_wav2vec_train)\n",
        "y_hat_test = predict_svm(model, X_wav2vec_test)\n",
        "train_acc = accuracy_score(y_train, y_hat_train)\n",
        "test_acc = accuracy_score(y_test, y_hat_test)\n",
        "print(f'Train accuracy: {train_acc}')\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "xIpTjegNq4RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HUBERT"
      ],
      "metadata": {
        "id": "TougC5Jj5vT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[(blog)](https://ai.facebook.com/blog/hubert-self-supervised-representation-learning-for-speech-recognition-generation-and-compression/) [(paper)](https://arxiv.org/abs/2106.07447)"
      ],
      "metadata": {
        "id": "3ABPWiAcHVMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1r6RQlOjs3RjEPd7xVa-mC1ds8FTomh9s\n",
        "!gdown --id 1eMnKrfGi3OB3dKcEMHLKJpbsztUvSDA7"
      ],
      "metadata": {
        "id": "IUrDl-39o2Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_hubert_train = np.load('./google_hubert_train.npy')\n",
        "X_hubert_test = np.load('./google_hubert_test.npy')\n",
        "print(f'Train shape: {X_hubert_train.shape}')\n",
        "print(f'Test shape: {X_hubert_test.shape}')"
      ],
      "metadata": {
        "id": "N61xVtLFpXLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below establishes a baseline model training and evaluation experiment using wav2vec features. You may use this as a comparison point for performance, and a starting point for integrating these features into ideas or other approaches you explored in Week 1 for this dataset."
      ],
      "metadata": {
        "id": "Zljo18ExUTY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_svm(X_hubert_train, y_train)\n",
        "y_hat_train = predict_svm(model, X_hubert_train)\n",
        "y_hat_test = predict_svm(model, X_hubert_test)\n",
        "train_acc = accuracy_score(y_train, y_hat_train)\n",
        "test_acc = accuracy_score(y_test, y_hat_test)\n",
        "print(f'Train accuracy: {train_acc}')\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "yxGCfOfpq_iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Try to see if you can achieve near ~100% test accuracy using more complex models.*"
      ],
      "metadata": {
        "id": "_YLXwCIoLfEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Research Notebook"
      ],
      "metadata": {
        "id": "tEtSGDmzUNOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sections above outline loading foundation model features, and establishing baseline performance using these features on each task. Your main task for this project is to continue improving performance on one of the challenge datasets using what you developed in Week 1, along with your new modeling tool -- foundation features!\n",
        "\n",
        "As usual, develop in build-measure-learn loops guided by a hypothesis of how you're improving the model or testing an idea at each iterative step."
      ],
      "metadata": {
        "id": "rngvmGgXURki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "\n",
        "#############################"
      ],
      "metadata": {
        "id": "Vi-lvB-nUQZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}