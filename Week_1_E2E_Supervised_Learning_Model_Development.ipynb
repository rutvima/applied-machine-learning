{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Coh4Bco0pYr4"
      },
      "source": [
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
        "\n",
        "# Appled ML SOTA Week 1: End-to-end Supervised Learning Model Development\n",
        "\n",
        "In this project, you will be building a supervised learning model on a single Kaggle dataset scraped from [Wish](https://www.wish.com/), an e-commerce website. Unlike previous weeks, where cleaned, complete datasets were provided for you, you will be working on designing an ML project end-to-end.\n",
        "\n",
        "This assignment will walk you through the steps to prepare a raw web scrape into a dataset that can be used to train a supervised learning model. You will also have the opportunity to try new state-of-the-art modeling approaches and apply what you have learned in practice.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. We provide starter code and data to give your work a common starting point and structure. You must keep function signatures unchanged to support later usage and to ensure your project is graded successfully.\n",
        "2. Read through the document and starting code before beginning your work. Understand the overall structure and goals of the project to ensure your implementation is efficient.\n",
        "4. Tasks marked as _extensions_ or _optional_ are intended to provide advanced ML engineering or modeling challenges. You may skip or attempt these tasks as you like.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szrT-OTntBcG"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "Let's start by importing all the libraries that we'll need throughout the project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzm-wAmOpSVW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from typing import Set, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Testing\n",
        "%pip install -U ipytest\n",
        "import ipytest\n",
        "import pytest\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3Do2xBwuySI"
      },
      "source": [
        "Next, we'll fix a random seed so we produce consistent results that can be easily discussed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwBh1dDquw6P"
      },
      "outputs": [],
      "source": [
        "# Fix the random seed so that we get consistent results\n",
        "# We'll use this same seed throughout the notebook\n",
        "SEED = 0\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZP4WXibwNcT"
      },
      "source": [
        "# Summer clothing sales prediction\n",
        "\n",
        "The raw Kaggle dataset we will be working with was gathered from Wish.com, an e-commerce website. Each sample in the dataset corresponds to a product that would appear if you type \"summer\" in the search field of the website. [Here is the Kaggle link.](https://www.kaggle.com/datasets/jmmvutu/summer-products-and-sales-in-ecommerce-wish?datasetId=819786&searchQuery=sales+prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following screenshot from the website shows some features and how to interpret them:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1yH00henYvw0h84lx0UyZGskPhkkKeQXP)"
      ],
      "metadata": {
        "id": "ncKJq2Qrx140"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVQqCMYZZKte"
      },
      "source": [
        "Given the attributes (or features) of a product, can we predict the number of units sold for that product? Let's load the dataset and formulate the problem as an ML task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3lzrNy6x6v7"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "We store the dataset on GDrive. This code downloads the dataset to your colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBoKwRXtsG7c"
      },
      "outputs": [],
      "source": [
        "!gdown 1NOzxjbZIiVc31V1_GEyXbMo4n4AuvpH4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M29jD9j4ycL2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('summer-products-with-rating-and-performance_2020-08.csv')\n",
        "print(\"Dataset size:\", len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEHSNLMK1fMb"
      },
      "source": [
        "Now, let's split the dataset into train and test sets. Since we are trying to predict the number of units sold, the `units_sold` column will become our $y$ values.\n",
        "\n",
        "The next cell separates the input ($X$) matrix and output ($y$) vector and splits\n",
        "the train and test sets. *We'll keep these splits fixed for the rest of the notebook to avoid leakage between train and test sets.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijpo1eB_1aDT"
      },
      "outputs": [],
      "source": [
        "# Make `units_sold` the y values\n",
        "X = df.drop('units_sold', axis=1)\n",
        "y = df['units_sold'].values\n",
        "\n",
        "# Get train and test set splits - don't modify these!\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByuhM2V8zDoa"
      },
      "source": [
        "## ML Task Formulation\n",
        "\n",
        "Recall that we want to design an ML model that can predict the number of units sold for the [Wish](https://www.wish.com/) product listings. At first glance, this naturally sounds like a regression problem.\n",
        "\n",
        "The small sample of product data above is showing that the values for `units_sold` are rounded numbers. Let's take a closer look at the distribution of `units_sold` to get a better understanding of what we're predicting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VNCCdzd1YJe"
      },
      "outputs": [],
      "source": [
        "# How many unique values for `units_sold` do we have?\n",
        "set(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJUmTDen8csJ"
      },
      "source": [
        "There are only 14 unique values for `units_sold`! This makes sense, since the Kaggle page describes this column as a \"lower bound approximation by steps.\" In other words, this isn't truly a continuous column, since the number of units sold have been put into bins. It would be better to formulate this as a **classification** problem, where we'll treat the different bins as classes.\n",
        "\n",
        "We formalize this classification task as:\n",
        "* Inputs $x$: attributes of the product listing, including `title`, `price`, `rating`, etc. (these may change as we do some feature engineering on the dataset later in this notebook!)\n",
        "* Output $y$: one of 10 classes indicating the number of units sold\n",
        "\n",
        "Let's convert the `units_sold` column into a column of 10 classes and build an initial model $f_θ(x) → y$. We'll combine the small values (where $y < 10$) into the same class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdToyu2L_6mP"
      },
      "outputs": [],
      "source": [
        "def get_units_sold_class(units_sold: int) -> str:\n",
        "    \"\"\"\n",
        "    Get the class for the given value of units_sold.\n",
        "    There are 10 distinct classes.\n",
        "    Args:\n",
        "      units_sold (int): original units_sold value from\n",
        "      the Kaggle dataset\n",
        "    Returns:\n",
        "      units_sold_class (string): string representation of the\n",
        "      class the given value of units_sold belongs to\n",
        "    \"\"\"\n",
        "    if units_sold < 10:\n",
        "        return '1-10'\n",
        "    elif units_sold < 50:\n",
        "        return '10-50'\n",
        "    elif units_sold < 100:\n",
        "        return '50-100'\n",
        "    elif units_sold < 1000:\n",
        "        return '100-1000'\n",
        "    elif units_sold < 5000:\n",
        "        return '1000-5000'\n",
        "    elif units_sold < 10000:\n",
        "        return '5000-10000'\n",
        "    elif units_sold < 20000:\n",
        "        return '10000-20000'\n",
        "    elif units_sold < 50000:\n",
        "        return '10000-50000'\n",
        "    elif units_sold < 100000:\n",
        "        return '50000-100000'\n",
        "    else:\n",
        "        return '>100000'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC8UysBiQt9v"
      },
      "source": [
        "### Task: Convert outputs into classes\n",
        "\n",
        "Using the `get_units_sold_class` function defined above, convert `y_train` and `y_test` into vectors of classes, represented by strings (`'1-10'`, `'10-50'`, etc.). Store the new vectors in variables called `y_train_class` and `y_test_class`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDcg9Gg-RGvm"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "# Store your new output vectors with the following variable names:\n",
        "# y_train_class = ...\n",
        "# y_test_class = ...\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test: Convert outputs into classes\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "@pytest.mark.parametrize('y, y_class, split', [\n",
        "    (y_train, y_train_class, 'train'),\n",
        "    (y_test, y_test_class, 'test'),\n",
        "])\n",
        "def test_y_class(y, y_class, split):\n",
        "    for i, (yi, yi_class) in enumerate(zip(y, y_class)):\n",
        "        assert get_units_sold_class(yi) == yi_class, \\\n",
        "            'Incorrect class assignment for index {} of y_{}_class'.format(i, split)"
      ],
      "metadata": {
        "id": "TzY6u2GcGZIk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lwaD9MUDVEM"
      },
      "source": [
        "## Classification Baseline\n",
        "\n",
        "Let's predict the majority class and report its test set accuracy. This will serve as our classification baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSGu5RfCJdom"
      },
      "outputs": [],
      "source": [
        "# Let's plot a bar chart of the frequencies for each class\n",
        "fig, ax = plt.subplots()\n",
        "units_sold_classes = pd.DataFrame({'units_sold_class': y_train_class})\n",
        "units_sold_classes.value_counts().plot(ax=ax, kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep227XN-KOik"
      },
      "outputs": [],
      "source": [
        "# Predict majority class\n",
        "y_train_pred = ['100-1000'] * len(y_train_class)\n",
        "y_test_pred = ['100-1000'] * len(y_test_class)\n",
        "\n",
        "print(\"Test accuracy: %.2f\" % accuracy_score(y_test_class, y_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXXv9jW-NXb3"
      },
      "source": [
        "# MVP Model\n",
        "Now that we've loaded the dataset, formulated the appropriate ML task, and performed a basic data check, let's get our first model working. The dataset doesn't come prepared in a format that can be fed into a model right off-the-bat, so we'll need to do some preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX-EsUjj_GtE"
      },
      "outputs": [],
      "source": [
        "# Get a fresh copy of the train and test set features\n",
        "# If we make any changes, we'll make them on this copy\n",
        "X_train_ft = X_train.copy()\n",
        "X_test_ft = X_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZmQSIISk8N8"
      },
      "source": [
        "## One-hot Encoding and Missing Value Imputation\n",
        "\n",
        "The inputs to a model are represented by the real-valued feature matrix $X$. This means that any string columns need to be transformed into numeric values in the feature matrix. We'll make the assumption that we can separate continuous and categorical variables based on their datatype; any numeric columns will be considered as continous variables, and any string columns will be considered as categorical variables. That way, we can one-hot encode string columns in order to transform them in the feature matrix. (This is a naive solution - we'll soon see some better feature engineering approaches this dataset.)\n",
        "\n",
        "We'll also need to consider any null values, which can't be given as inputs to the model. We'll just fill those with 0 for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ3UNcoPQAQc"
      },
      "outputs": [],
      "source": [
        "def get_columns(df: pd.DataFrame) -> Tuple[pd.Index, pd.Index]:\n",
        "    \"\"\"\n",
        "    Get the continuous and categorical columns from the given\n",
        "    DataFrame df based on the columns' datatype. Numerical columns\n",
        "    (dtype float64, int64) are considered continuous, and string columns\n",
        "    (dtype object) are considered categorical.\n",
        "    Args:\n",
        "      df (DataFrame): original dataset features\n",
        "    Returns:\n",
        "      (continuous_columns, categorical_columns): tuple containing\n",
        "      continuous and categorical columns from the input df\n",
        "    \"\"\"\n",
        "    continuous_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    return continuous_columns, categorical_columns\n",
        "\n",
        "continuous_columns, categorical_columns = get_columns(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxZppr4S56Fl"
      },
      "source": [
        "### Task: Simple preprocessing using Pandas\n",
        "\n",
        "One-hot encode categorical columns and impute missing values in the dataset using Pandas functions (i.e. `get_dummies` and `fillna`).\n",
        "Assign the new Dataframe to a variable `X`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za-77QH73ZYI"
      },
      "outputs": [],
      "source": [
        "# One-hot encode categorical columns and fill null values with zero\n",
        "# When using get_dummies, the X_train and X_test need to be\n",
        "# encoded simultaneously in order to have the same number of columns\n",
        "num_train = len(X_train)\n",
        "X = pd.concat([X_train, X_test])\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test: Simple preprocessing using Pandas\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "def test_dtypes():\n",
        "    assert(np.dtype('object') not in set(X.dtypes)), \\\n",
        "        '''Dataframe contains non-numeric column. Check your one-hot encoding.'''\n",
        "\n",
        "def test_column_count():\n",
        "    assert(len(X.columns) == 11969), \\\n",
        "        '''Dataframe has the incorrect number of columns. Check your one-hot encoding.'''\n",
        "\n",
        "def test_null_entries():\n",
        "    for col, sum in X.isnull().sum().items():\n",
        "        assert(sum == 0), '''Column {} contains {} non-null entries'''.format(col, sum)"
      ],
      "metadata": {
        "id": "ykb-QQDuJQ9m",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_1ZoGueR8tw"
      },
      "outputs": [],
      "source": [
        "# Separate out X_train_ft and X_test_ft\n",
        "X_train_ft = X.iloc[:num_train].copy()\n",
        "X_test_ft = X.iloc[num_train:].copy()\n",
        "\n",
        "# Check for null values\n",
        "print('Null values in train set:', X_train_ft.isnull().sum())\n",
        "print('Null values in test set:', X_train_ft.isnull().sum())\n",
        "\n",
        "# Check the new number of features\n",
        "print('Train set shape:', X_train_ft.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oijLoqYXhwAJ"
      },
      "source": [
        "## Linear Model Test\n",
        "Now that we've done this preprocessing, we can train a scikit-learn model from our dataset. Let's try logisitc regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRo3DqXTepNx"
      },
      "outputs": [],
      "source": [
        "# Fit linear model\n",
        "lr = linear_model.LogisticRegression(max_iter=500)\n",
        "lr.fit(X_train_ft, y_train_class)\n",
        "\n",
        "print(\"Train accuracy: %.2f\" % accuracy_score(y_train_class, lr.predict(X_train_ft)))\n",
        "print(\"Test accuracy: %.2f\" % accuracy_score(y_test_class, lr.predict(X_test_ft)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mckSPjZpiKTT"
      },
      "source": [
        "The logistic regression model performs better than our baseline, but accuracy is still quite low on both the training and test sets. We also get a convergence warning, even when setting a large number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfiZheNCpZqX"
      },
      "source": [
        "## Coordinate Standardization\n",
        "\n",
        "One crucial preprocessesing step we are missing is coordinate standardization for the continuous features. Let's take a look at the mean and standard deviation of these features in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIwZEATjnRG2"
      },
      "outputs": [],
      "source": [
        "# Get stats about continuous features using .describe()\n",
        "X_train_ft[continuous_columns].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Db0y-fn-1f"
      },
      "source": [
        "ML models work best when all the features have a mean of 0 and a standard deviation of 1, but this is far from true for our dataset right now. Let's z-score our dataset and see how it improves the linear model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNNj2rj1JY4H"
      },
      "source": [
        "### Task: Scaling continuous columns\n",
        "The code block below uses scikit's built-in `StandardScaler` to z-score the continuous columns in the training dataset. Notice that the `fit_transform()` function serves to both \"fit\" the scaler to the training dataset and perform the transformation simultaneously. Your task is to transform the test set using the fitted `scaler`. Store the transformed test set in a variable called `X_test_ft`.\n",
        "\n",
        "You can read more about the usage of `StandardScaler` in the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY_WQ4Vwn-iG"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Separate columns\n",
        "encoded_columns = list(set(X_train_ft.columns)-set(continuous_columns))\n",
        "X_train_encoded_cols = X_train_ft[encoded_columns]\n",
        "X_train_scaled_cols = scaler.fit_transform(X_train_ft[continuous_columns])\n",
        "X_train_ft = np.concatenate([X_train_scaled_cols, X_train_encoded_cols], axis=1)\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test: Scaling continuous columns\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "def test_feature_cnt():\n",
        "    assert(X_train_ft.shape[1] == X_test_ft.shape[1]), \\\n",
        "        '''Train and test sets have a mismatch in the number of features'''"
      ],
      "metadata": {
        "id": "tCaiurKoyYAG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will fit a few models on the normalized dataset."
      ],
      "metadata": {
        "id": "Cx5sm2qyyJxy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBIU2Baqstnw"
      },
      "outputs": [],
      "source": [
        "# Fit linear model\n",
        "lr = linear_model.LogisticRegression(max_iter=500)\n",
        "lr.fit(X_train_ft, y_train_class)\n",
        "\n",
        "print(\"Train accuracy: %.2f\" % accuracy_score(y_train_class, lr.predict(X_train_ft)))\n",
        "print(\"Test accuracy: %.2f\" % accuracy_score(y_test_class, lr.predict(X_test_ft)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo3wQrlOEECL"
      },
      "outputs": [],
      "source": [
        "# Try nonlinear model\n",
        "dt = DecisionTreeClassifier(random_state=SEED)\n",
        "dt.fit(X_train_ft, y_train_class)\n",
        "\n",
        "print(\"Train accuracy: %.2f\" % accuracy_score(y_train_class, dt.predict(X_train_ft)))\n",
        "print(\"Test accuracy: %.2f\" % accuracy_score(y_test_class, dt.predict(X_test_ft)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwIB_DOttgHh"
      },
      "source": [
        "After z-scoring the dataset, we no longer get a convergence warning while fiting the logistic regression model, and we get a more reasonable test accuracy. However, we can see that the model is overfitting the training set since it achieves perfect train accuracy.\n",
        "\n",
        "**Question:**\n",
        "Why do you think a simple linear model like logistic regression is overfitting the training set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-H2QvxG9fif"
      },
      "source": [
        "===== (Write your answer here) ====="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFa8nukC0_2e"
      },
      "outputs": [],
      "source": [
        "X_train_ft.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rurq2Eq41Aik"
      },
      "source": [
        "Our transformed feature matrix for the training set has 11,969 features for just 1,258 training samples! As a general rule of thumb, the ratio of samples to features should be about 10:1. In our case, our feature matrix is far from ideal -- there are way too many features for a small number of training examples.\n",
        "\n",
        "Before trying any more complex models to improve test performance, let's take a closer look at our dataset in order to engineer better features for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3VsB-ZzzFqo"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "In order to perform principled feature engineering, we need to get a better understanding of what our dataset actually contains. We'll see that our naive choices for continuous and categorical columns were not ideal, and some columns will require hands-on feature derivations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2AaN9Y_-rXh"
      },
      "outputs": [],
      "source": [
        "# Get a fresh copy of the train and test set features\n",
        "# If we make any changes, we'll make them on this copy\n",
        "X_train_ft = X_train.copy()\n",
        "X_test_ft = X_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yebEnsET0hcm"
      },
      "source": [
        "## Dataset Understanding\n",
        "\n",
        "Let's take a closer look at all of the columns in our original training dataset, before we performed any transformations. Pandas's `.info()` function provides the row counts, column names, non-null counts for each column, and dtype for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHJVqySXzFUi"
      },
      "outputs": [],
      "source": [
        "# Get info for each column\n",
        "X_train_ft.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8Y4cntz1wwI"
      },
      "outputs": [],
      "source": [
        "# Observe first example in dataset\n",
        "X_train_ft.iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc1dP3TNhwL8"
      },
      "source": [
        "Under the CSV preview, the Kaggle link also includes column descriptions that can aid us with feature engineering:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=10lW-pwE91IOZJuhHK5u-Z5ouYgvo_JOu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcve1UWkiROL"
      },
      "source": [
        "Here are the column descriptions:\n",
        "1. Title: localized for european countries. May be the same as title_orig if the seller did not offer a translation\n",
        "2. Title_orig: original english title of the product\n",
        "3. price: price for buyer\n",
        "4. retail_price: retail price, or reference price in other stores/places. Used by the seller to indicate a regular value or the price before discount.\n",
        "5. currency_buyer\n",
        "6. units_sold: Number of units sold. Lower bound approximation by steps\n",
        "7. uses_ad_boosts: Whether the seller paid to boost his product within the platform (highlighting, better placement or whatever)\n",
        "8. rating: Mean product rating\n",
        "9. rating_count: Total number of ratings of the product\n",
        "10. rating_five_count: Number of 5-star ratings\n",
        "11. rating_four_count: Number of 4-star ratings\n",
        "12. rating_three_count: Number of 3-star ratings\n",
        "13. rating_two_count: Number of 2-star ratings\n",
        "14. rating_one_count: Number of 1-star ratings\n",
        "15. badges_count: number of badges the product or seller have\n",
        "16. badges_local_product: A badge that denotes the product is a local product. Conditions may vary (being produced locally, or something else). Some people may prefer buying local products rather than. 1 means Yes, has the badge\n",
        "17. badge_product_quality: Badge awarded when many buyers consistently gave good evaluations 1 means Yes, has the badge\n",
        "18. badge_fast_shipping: Badge awarded when this product's order is consistently shipped rapidly\n",
        "19. tags: tags set by the seller\n",
        "20. product_color: Product's main color\n",
        "21. product_variation_size_id: One of the available size variation for this product\n",
        "22. product_variation_inventory: Inventory the seller has. Max allowed quantity is 50\n",
        "23. shipping_option_name\n",
        "24. shipping_option_price: shipping price\n",
        "25. shipping_is_express: whether the shipping is express or not. 1 for True\n",
        "26. countries_shipped_to: Number of countries this product is shipped to. Sellers may choose to limit where they ship a product to\n",
        "27. inventory_total: Total inventory for all the product's variations (size/color variations for instance)\n",
        "28. has_urgency_banner: whether there was an urgency banner with an urgency\n",
        "29. urgency_text: A text banner that appear over some products in the search results.\n",
        "30. origin_country\n",
        "31. merchant_title: Merchant's displayed name (show in the UI as the seller's shop name)\n",
        "32. merchant_name: Merchant's canonical name. A name not shown publicly. Used by the website under the hood as a canonical name. Easier to process since all lowercase without white space\n",
        "33. merchant_info_subtitle: The subtitle text as shown on a seller's info section to the user. (raw, not preprocessed). The website shows this to the user to give an overview of the seller's stats to the user. Mostly consists of \"% ( reviews)\" written in french\n",
        "34. merchant_rating_count: Number of ratings of this seller\n",
        "35. merchant_rating: merchant's rating\n",
        "36. merchant_id: merchant unique id\n",
        "37. merchant_has_profile_picture: Convenience boolean that says whether there is a \"merchant_profile_picture\" url\n",
        "38. merchant_profile_picture: Custom profile picture of the seller (if the seller has one). Empty otherwise.\n",
        "39. product_url: url to the product page. You may need to login to access it\n",
        "40. product_picture\n",
        "41. product_id: product identifier. You can use this key to remove duplicate entries if you're not interested in studying them.\n",
        "42. theme: the search term used in the search bar of the website to get these search results.\n",
        "43. crawl_month: meta for info only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUixW3yi9zQs"
      },
      "source": [
        "**Question:**\n",
        "\n",
        "Based on what we know so far about the dataset, what columns do you think are important for predicting the number of units sold? What columns do you think are irrelevant? Feel free to consult the additional metadata about the dataset on the Kaggle page!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG9218Hg-N0_"
      },
      "source": [
        "===== (Write your answer here) ====="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPDtBOyx5EMR"
      },
      "source": [
        "## Re-evaluating Column Encodings\n",
        "\n",
        "Observing the info table and the example, we can see that some of the string columns we treated as categorical features are actually free-form text, such as `title`, `title_orig`, `merchant_info_subtitle`, etc. These are likely to be unique to each example and shouldn't be treated as true classes.\n",
        "\n",
        "Let's see if this is the case by checking the number of unique values for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e08v4xtmBVql"
      },
      "outputs": [],
      "source": [
        "X_train_ft.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3yuXyBJB1bI"
      },
      "source": [
        "Many of the string columns have a large number of unique values. Out of the 1,415 samples in the training set, there are 1,100 unique values for `title`. One-hot encoding columns such as the `title` blows up the number of features in our dataset, and since it is unlikely that the test set will exactly match an existing `title` in the training set, we won't glean any meaningful generalization from those features. This is true, not only for free-form text columns, but also for columns containing `ids` or `urls`. There are separate NLP methods to handle free-form text, but for now, let's omit all these columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k5Ha_LpC6Nr"
      },
      "outputs": [],
      "source": [
        "# Drop long-form text-based columns\n",
        "text_cols = ['title', 'title_orig', 'merchant_title', 'merchant_name', 'merchant_info_subtitle']\n",
        "\n",
        "# Drop IDs, usernames, URLs, etc.\n",
        "id_cols = ['merchant_id', 'merchant_profile_picture', 'product_picture', 'product_url', 'product_id']\n",
        "\n",
        "categorical_cols_to_drop = text_cols + id_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39-6dk-E3pZ"
      },
      "source": [
        "There are also several columns that only have a single unique value. A model won't learn any meaningful variation between samples from such features, since all samples have the same value. We can omit these columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uERsjteSFbYF"
      },
      "outputs": [],
      "source": [
        "# Columns with only one unique value\n",
        "single_val_cols = ['currency_buyer', 'theme', 'crawl_month']\n",
        "categorical_cols_to_drop.extend(single_val_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln-777rbJeMY"
      },
      "source": [
        "You may have noticed that we did not choose to drop `has_urgency_banner` and `urgency_text`. Those columns actually have two unique values - a null value and a non-null value. A null value indicates that no urgency banner was present on the listing. However, these two columns essentially encode the same attribute and will be completely correlated, so let's drop one of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2_Z5m-dJptS"
      },
      "outputs": [],
      "source": [
        "# Drop urgency text since it's redundant\n",
        "categorical_cols_to_drop.append('urgency_text')\n",
        "\n",
        "# Now, let's drop all these columns\n",
        "X_train_ft.drop(columns=categorical_cols_to_drop, inplace=True)\n",
        "X_test_ft.drop(columns=categorical_cols_to_drop, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88RIBp5td2ys"
      },
      "source": [
        "## Derived Features\n",
        "\n",
        "Some columns can be transformed into useful derived features. In the dataset, the `tags` column is a list keywords set by the seller. We can extract the most frequent tags in the training set and use that to encode a new set of boolean features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKpVjd_Ld2kn"
      },
      "outputs": [],
      "source": [
        "# View some tags in the training set\n",
        "X_train_ft['tags'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_2wcSKYSyxr"
      },
      "source": [
        "### Task: Find the most frequent tags in the training set\n",
        "\n",
        "Extract the most frequent tags across the training set. Tags for a given item are comma-separated. For example, in the samples above, `Mini`, `womens dresses`, and `Summer` are different tags. Some points to consider:\n",
        "- Tags in the dataset are comma separated.\n",
        "- You should do some case normalization -- `shirt` and `Shirt` should be considered the same tag.\n",
        "- Use python's `Counter` to keep counts of the tags, and store the counter in a variable `c`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll0oR2pqnwL1"
      },
      "outputs": [],
      "source": [
        "c = Counter()\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################\n",
        "\n",
        "# View top tags\n",
        "c.most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test: Find the most frequent tags\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "top_tags = [\n",
        "  \"women's fashion\",\n",
        "  'summer',\n",
        "  'fashion',\n",
        "  'women',\n",
        "  'casual',\n",
        "  'plus size',\n",
        "  'sleeveless',\n",
        "  'shorts',\n",
        "  'dress',\n",
        "  'tops',\n",
        "  'sexy',\n",
        "  'beach',\n",
        "  'print',\n",
        "  'short sleeves',\n",
        "  'sleeve',\n",
        "  'shirt',\n",
        "  'tank',\n",
        "  'necks',\n",
        "  'printed',\n",
        "  't shirts',\n",
        "]\n",
        "\n",
        "@pytest.mark.parametrize('tag', list(zip(*c.most_common(20)))[0])\n",
        "def test_top_tags(tag):\n",
        "    assert(tag in top_tags)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cm1OTl9tTtG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task: Create tag-based features\n",
        "\n",
        "Now that you've created a counter with the most frequent tags, it's time to derive a new set of columns from the most frequent tags. Engineer a new set of features based on the 20 most common tags. The new feature columns should be named after the tag and have a prefix `tag_`.\n",
        "\n",
        "For example, if the tag is \"summer\", the new feature column should be called `tag_summer`. For a given sample (i.e. row in the dataset), if \"summer\" is one of the tags in the `tags` column, then the value of the `tag_summer` column should be 1. Otherwise, it should be 0. You can think of this as an *indicator* feature.\n",
        "\n",
        "There should be a total of 20 additional feature columns for the 20 tags. Make sure to add the columns to both the training and the test set features."
      ],
      "metadata": {
        "id": "RnfxQqq8cZr5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7KoCSp275CQ"
      },
      "outputs": [],
      "source": [
        "# Get 20 most common tags\n",
        "common_tags = [tag for tag, _ in c.most_common(20)]\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################\n",
        "\n",
        "# Drop old tags column\n",
        "X_train_ft.drop('tags', axis=1, inplace=True)\n",
        "X_test_ft.drop('tags', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSZl6_9y8Eow"
      },
      "outputs": [],
      "source": [
        "# Sanity check a few tags' features\n",
        "print(\"'summer' count:\", len(X_train_ft[X_train_ft['tag_summer'] == 1])) # Should be 1055\n",
        "print(\"'sleeve' count:\", len(X_train_ft[X_train_ft['tag_sleeve'] == 1])) # Should be 227\n",
        "print(\"'tank' count:\", len(X_train_ft[X_train_ft['tag_tank'] == 1])) # Should be 211"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test: Tag feature counts\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "@pytest.mark.parametrize('tag', top_tags)\n",
        "def test_top_tags(tag):\n",
        "    assert(len(X_train_ft[X_train_ft['tag_'+tag] == 1]) == c[tag])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FJg5jmuocORQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDMiJHqeW00C"
      },
      "source": [
        "Another column that can be encoded into a better set of derived features is the `product_color` column. When we one-hot encode this column, variants of the same color are completely orthogonal to each other, i.e. there is no indication that `green` and `armygreen` are similar. Since there are so many color variants in the dataset, let's consolidate the different color categoreis and encode a smaller set of features based on the most common colors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIfXcJFcXGoQ"
      },
      "outputs": [],
      "source": [
        "color_counts = X_train_ft['product_color'].value_counts()\n",
        "color_counts[color_counts > 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf1wtPoFrIIH"
      },
      "source": [
        "Some of the common color variants are represented by entirely different words. For instance, `beige` is a variant of `brown`. We'll have to consolidate those manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjK_7hGIpj3o"
      },
      "outputs": [],
      "source": [
        "# Manually consolidate color variants\n",
        "X_train_ft['product_color'].replace('beige', 'brown', inplace=True)\n",
        "X_test_ft['product_color'].replace('beige', 'brown', inplace=True)\n",
        "X_train_ft['product_color'].replace('coffee', 'brown', inplace=True)\n",
        "X_test_ft['product_color'].replace('coffee', 'brown', inplace=True)\n",
        "X_train_ft['product_color'].replace('rose', 'pink', inplace=True)\n",
        "X_test_ft['product_color'].replace('rose', 'pink', inplace=True)\n",
        "\n",
        "# Replace occurrences of 'gray' with its common spelling 'grey'\n",
        "X_train_ft['product_color'].replace('gray', 'grey', inplace=True)\n",
        "X_test_ft['product_color'].replace('gray', 'grey', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxssuLUGmDQO"
      },
      "outputs": [],
      "source": [
        "common_colors = [\n",
        "    'black', 'white', 'yellow', 'blue', 'pink', 'brown',\n",
        "    'red', 'green', 'grey', 'purple', 'orange'\n",
        "]\n",
        "\n",
        "# Let's engineer a new set of features based on this list of colors\n",
        "for color in common_colors:\n",
        "  X_train_ft['color_'+color] = X_train_ft.apply(\n",
        "      lambda x: int(color in str(x['product_color']).lower()),\n",
        "      axis = 1\n",
        "  )\n",
        "  X_test_ft['color_'+color] = X_test_ft.apply(\n",
        "      lambda x: int(color in str(x['product_color']).lower()),\n",
        "      axis = 1\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XuBMibtYLmS"
      },
      "outputs": [],
      "source": [
        "# Sanity check colors\n",
        "X_train_ft[['product_color', 'color_white', 'color_black', 'color_red', 'color_green', 'color_grey', 'color_pink']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI-w07Hwuev8"
      },
      "outputs": [],
      "source": [
        "# Drop old product_color column\n",
        "X_train_ft.drop('product_color', axis=1, inplace=True)\n",
        "X_test_ft.drop('product_color', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCEMjmH1xbG_"
      },
      "source": [
        "## Remove Correlated Features\n",
        "\n",
        "Next, let's check on whether the continuous columns in the dataset have any correlations with each other. A group of highly correlated features does not contribute new information for the model to learn, and can even cause [linear models to produce wildly varying solutions](https://en.wikipedia.org/wiki/Multicollinearity#Consequences_of_multicollinearity) from small changes to the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJPM0dVQ_kCb"
      },
      "source": [
        "### Task: Visualize Correlated Feature Matrix\n",
        "\n",
        "The pandas DataFrame function `.corr()` produces a correlation matrix that we can plot to easily visualize the correlations. Plot a heatmap of the correlation matrix to visualize correlated features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QINsGwn8t-t"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CXL5tNOA4_-"
      },
      "source": [
        "The heatmap shows that there are several features related to the product rating that are correlated with each other. The columns `rating_five_count`,\t`rating_four_count`, etc. are all correlated, and `rating` and `rating_count` can be derived directly from those columns as well. Let's remove the redundant columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkPmGY0sxauQ"
      },
      "outputs": [],
      "source": [
        "# Remove redundant correlated features\n",
        "drop_cols = ['rating_five_count',\t'rating_four_count', 'rating_three_count', 'rating_two_count', 'rating_one_count']\n",
        "X_train_ft.drop(columns=drop_cols, inplace=True)\n",
        "X_test_ft.drop(columns=drop_cols, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6uc1BDRFIes"
      },
      "source": [
        "**Question:**\n",
        "\n",
        "Are there any other features you would remove or transform in the dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M--E6W9OFerm"
      },
      "source": [
        "===== (Write your answer here) ====="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHj8gwQjWvmc"
      },
      "source": [
        "## Built-in Preprocessing Steps\n",
        "\n",
        "Now that we've done some principled feature engineering, we will finish by completing the required steps of one-hot encoding, scaling, and missing value imputation as we had done previously.\n",
        "\n",
        "You might have noticed that performing these preprocessing steps can get quite repetitive for any new data that we would like to pass into our model. Fortunately, scikit-learn provides several built-in functions that can do these preprocessing steps for us -- in particular, `Pipeline` is very useful for cleaning up code and collapsing all preprocessing and modeling steps into to a single line of code.\n",
        "\n",
        "We already used `StandardScaler` to z-score continuous columns, but scikit also provides built-ins like `OneHotEncoder` and `SimpleImputer` to perform one-hot encoding and missing value imputation that we previously computed using Pandas functions.\n",
        "\n",
        "\n",
        "Let's try these out rather than performing the preprocessing steps manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hTrXQIdIKfg"
      },
      "outputs": [],
      "source": [
        "# Get new list of continuous and categorical columns\n",
        "continuous_columns, categorical_columns = get_columns(X_train_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgMWW4mvbnNz"
      },
      "source": [
        "### Task: Scikit-learn preprocessing with Pipelines\n",
        "Use scikit-learn's built-in `StandardScaler`, `OneHotEncoder`, and `SimpleImputer` to preprocess the dataset. Combine multiple preprocessing steps using the `Pipeline` class. `Pipeline` takes a list of tuples of transformers for its `steps` argument, where each tuple has the pattern `('name_of_transformer', transformer)`. Each step will be chained and applied to the passed DataFrame in the given order.\n",
        "\n",
        "We have already written a pipeline that transforms continuous features as an example. Write another pipeline that imputes and one-hot encodes categorical features. You can refer to the [documention on `Pipelines`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more examples, as well as [this tutorial](https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsSPLrytE65-"
      },
      "outputs": [],
      "source": [
        "# Instantiate scaler, encoder, & imputer\n",
        "scaler = StandardScaler()\n",
        "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "imputer = SimpleImputer()\n",
        "\n",
        "# Example of using Pipeline for numeric features\n",
        "numeric_pipeline = Pipeline(\n",
        "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")),\n",
        "           (\"scale\", StandardScaler())]\n",
        ")\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl_f5hh2Rttq"
      },
      "source": [
        "We'll now combine both pipelines using `ColumnTransformer`, which is similar to `Pipeline`, but it allows us to specify which columns to apply a transformation to. This creates one processor that can fit and transform the dataset in a single line of code.\n",
        "\n",
        "(Note: you can also add model instantiation as a part of a scikit-learn `Pipeline`. This means you can preprocess the dataset and train the model all in one line. This is shown in the tutorial linked above.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztRNttrERtU2"
      },
      "outputs": [],
      "source": [
        "full_processor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numeric\", numeric_pipeline, continuous_columns),\n",
        "        (\"categorical\", categorical_pipeline, categorical_columns),\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train_ft = full_processor.fit_transform(X_train_ft)\n",
        "X_test_ft = full_processor.transform(X_test_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSID6nYQDHMC"
      },
      "outputs": [],
      "source": [
        "# Fit linear model\n",
        "lr = linear_model.LogisticRegression(max_iter=500)\n",
        "lr.fit(X_train_ft, y_train_class)\n",
        "\n",
        "print(\"Train accuracy: %.2f\" % accuracy_score(y_train_class, lr.predict(X_train_ft)))\n",
        "print(\"Test accuracy: %.2f\" % accuracy_score(y_test_class, lr.predict(X_test_ft)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9qBCEdsDoi_"
      },
      "outputs": [],
      "source": [
        "# Number of features\n",
        "X_train_ft.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mSbnqALCorb"
      },
      "source": [
        "With the new feature set, the linear model no longer fits the training set perfectly.\n",
        "The test set accuracy has also reduced, but we were able to train a model with much fewer features (159), resulting in a much faster convergence time and less overfitting to the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE2Ax9kmyBNK"
      },
      "outputs": [],
      "source": [
        "# Try nonlinear model\n",
        "dt = DecisionTreeClassifier(random_state=SEED)\n",
        "dt.fit(X_train_ft, y_train_class)\n",
        "\n",
        "print(\"Train accuracy: %.2f\" % accuracy_score(y_train_class, dt.predict(X_train_ft)))\n",
        "print(\"Test accuracy: %.2f\" % accuracy_score(y_test_class, dt.predict(X_test_ft)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6srE0fleNvCJ"
      },
      "source": [
        "The non-linear decision tree model still overfits the training set, but we see a slight improvement in test set performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test: logistic regression and decision tree metrics\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "def test_lr_test_accuracy():\n",
        "    assert round(accuracy_score(y_test_class, lr.predict(X_test_ft))) > 0.55, \\\n",
        "       '''Logistic regression's test accuracy is low. Double check your encoding and model training.'''\n",
        "\n",
        "def test_dt_test_accuracy():\n",
        "    assert round(accuracy_score(y_test_class, dt.predict(X_test_ft))) > 0.70, \\\n",
        "       '''Decision tree's test accuracy is low. Double check your encoding and model training.'''"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oj-kxX2pg0N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIfYGB_Ezxu"
      },
      "source": [
        "# Improving Performance with SOTA Models\n",
        "\n",
        "Now that we've cleaned the dataset with a reduced set of derived features, let's try more SOTA classification models to improve test set performance. This week's [reference notebook](https://colab.research.google.com/drive/1uQF5cF1HnmhWIaze4gDMbN2XE4Ovh_X_?usp=sharing) lists several competition-winning models that often apply easily in practice.\n",
        "\n",
        "This part of the project is open-ended--it is up to you to choose whether you want to pursue advanced or optional techniques to improve performance. Our suggestion is that you spend a few hours to achieve the best model you can.\n",
        "\n",
        "**Keep track of your work**\n",
        "\n",
        "As you try different techniques, visualize data/results, and try side experiments, keep track of your code and experiments! It's okay to let your work contain models that helped you learn but were replaced in later experiments. Keeping a _research journal_ as you work will help you refer back to what you've tried, what works, and where you can improve further later. Keep track of your work here in case you talk through it with peers or teaching staff.\n",
        "\n",
        "**Note about using XGBoost**\n",
        "\n",
        "XGBoost is a gradient boosting decision tree library that has a nice interface that integrates with scikit-learn. To use XGBoost, you will need to convert the output classes from strings to integers. You can do this easily with `LabelEncoder`. (XGBoost used to use `LabelEncoder` under the hood for you, but this functionality has been removed in a recent update.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E0TCt_ia1Kl"
      },
      "outputs": [],
      "source": [
        "# SOTA models to try\n",
        "from sklearn.ensemble import \\\n",
        "  RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Highly optimized gradient boosting library\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vsu6Inswusd"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Parameter searching functions\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et1PVCCXaeCZ"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def timer(start_time=None):\n",
        "    '''\n",
        "    Helper function to keep track of training time.\n",
        "    Example usage:\n",
        "      start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
        "      ...\n",
        "      timer(start_time) # timing ends here for \"start_time\" variable\n",
        "    '''\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
        "        tmin, tsec = divmod(temp_sec, 60)\n",
        "        print('\\nTime taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmU2RU_wNQyM"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3HlWA28eIt3"
      },
      "source": [
        "## (Optional) Extension: Text and Image Features\n",
        "\n",
        "So far, we have only been using tabular features and have not been considering the text and images provided in the dataset. Each example has a title and an image that can provide more signals about the item's price. Let's see if including these additional features improves model performance.\n",
        "\n",
        "The next cells load pre-generated sentence features for the `title_orig` attribute and pre-generated image features for the `product_picture` attribute in the dataset using a *foundation model* called CLIP. Next week's lecture will be all about these models, but you can get a sneak peak into their capabilities in this extension. (We used a model client called `clip-as-service` to generate these features. You can look at [this reference notebook](https://colab.research.google.com/drive/1o0YSoZnWUgmy0SkLZbaj2DgBuT9n6sSr?usp=sharing) to see the exact code that generated the features.)\n",
        "\n",
        "You can use these precomputed features and see whether they improve test set performance. Alternatively, you can also try some of the traditional text and image preprocessing methods shown in the reference notebook this week using scikit's `CountVectorizer`/`TfidfVectorizer` for text and HOG features for images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyMVoInlbSV1"
      },
      "outputs": [],
      "source": [
        "# Load clip text features with gdown\n",
        "!gdown 1wRyNDxXGNT5KdNeIfo-p8xCJekZC7mlJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25IAYdflbZny"
      },
      "outputs": [],
      "source": [
        "loaded = np.load('clip_features.npz')\n",
        "\n",
        "# Get pre-computed text features\n",
        "X_train_text = loaded['X_train_text']\n",
        "X_test_text = loaded['X_test_text']\n",
        "\n",
        "# Get pre-computed image features\n",
        "X_train_image = loaded['X_train_image']\n",
        "X_test_image = loaded['X_test_image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMEQOuN2fVk6"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-RKQmhtlIMq"
      },
      "outputs": [],
      "source": [
        "print(\"Train accuracy: %.2f\" % accuracy_score(y_train_enc, random_search.predict(X_train_fts)))\n",
        "print(\"Test accuracy: %.2f\" % accuracy_score(y_test_enc, random_search.predict(X_test_fts)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAFAU8_Kgfx3"
      },
      "source": [
        "# Takeaways\n",
        "We don't do much in the way of formal grading, but you should prepare some experimental results, explanations of your experiments, and conclusions of your modeling work. Reporting what you tried and the outcomes you observed is a central part of quality ML engineering -- and it's critical for building successful ML systems when collaboration is involved. Here's some results and answers you should have ready when discussing your project:\n",
        "* What are some baseline methods and their performance on this task?\n",
        "* What modeling improvements did you try? How did each modeling improvement affect results (show a full results table if you can)\n",
        "* What is your best result? What combination of modeling/data tricks produced this result?\n",
        "* Did you perform any ablation or sensitivity experiments to understand which aspects of your best system are most important?\n",
        "* Error analysis: Have you visualized where your model makes mistakes? (Either in aggregate or with individual mistaken examples)\n",
        "* What is your current diagnosis of the ML System? Is it high variance/bias? What are your thoughts on current dataset size relative to model capacity / fit?\n",
        "* What might you try next to improve on this task? Could you improve with more data? More time spent building larger models? Data augmentation or similar techniques?\n",
        "* Can you identify cases or types of inputs where the model is likely to make mistakes? Are there gaps in the training set and/or model assumptions which would lead the model to make mistakes or not have sufficient data in certain situations?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}