{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive."
      ],
      "metadata": {
        "id": "rql5Fs5RyDux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference: State-of-the-art (SOTA) Supervised Learning Models\n",
        "\n",
        "To help you get started with state-of-the-art ML modeling approaches across many domains,  we will be introducing a variety of model classes you can try on challenging ML tasks. We won't cover deep learning approaches in this project, but many of the best practices and classifier tricks covered here can help improve even complex deep learning systems. As usual, advanced students are welcome to dig deeper into the advanced algorithmic ideas covered here.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. This notebook is simply a reference on advanced modeling ideas like Boosting and Bagging. Reading through these ideas in depth **is optional**. You can use only the summary below to see scikit models you can try to leverage these ideas in practice."
      ],
      "metadata": {
        "id": "Slul37dyyFYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary: SOTA Machine Learning Approaches\n"
      ],
      "metadata": {
        "id": "dg5-7m_NFawG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may be surprised to know that most Kaggle competitions are not won by deep neural networks. Instead, most are won by classic machine learning methods combined with clever featurization. [Here](https://www.ednetchallenge.ai/aied-challenge-1) is a good example: Riiid is a Korean education company that released a Kaggle competition along with a 1M row dataset of student answering academic questions. Their goal was to produce a ML model to understand student knowledge that could predict whether students would correctly answer new (unseen) questions. Although many deep neural network approaches were tried, many [top performing solutions](https://www.kaggle.com/ragnar123/riiid-model-lgbm) used classic machine learning methods (with features derived from deep neural networks), in particular a method called Gradient Boosting, which we cover below.\n",
        "\n",
        "Our goal in this section is introduce the main ideas behind SOTA models, and how to use these to produce SOTA results using the guiding themes of this course. We will introduce some key statistical concepts which tend to improve the performance of many model types: **Ensembling**, **Bagging**, and **Boosting**. These concepts combine with some model primitives you've already seen (linear models and decision trees) to form SOTA techniques for supervised ML.\n"
      ],
      "metadata": {
        "id": "Vgr_X6hp1FUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SOTA models to try in practice\n",
        "When achieving the best possible dev/test set performance is critical, just about any ML model can improve by \"merging\" multiple versions of a model.\n",
        "\n",
        "We cover three ways to \"merge\" models by combining different models together: ensembling, bagging, and boosting. Each of these have their individual strengths and weaknesses, and of course they can be combined. You can _ensemble boosted bagged models_ (what a mouthful!). While these are general statistical ML tools, usually we can rely on pre-built models which leverage these ideas for us.\n",
        "\n",
        "Here are some of the most popular algorithms which leverage and combine these techniques in different ways. The scikit documentation gives more detail on what these models do, and available hyper-parameters to tune when using these models.\n",
        "\n",
        "**TL;DR: You can try the models below on a range of classification and regression tasks, and often achieve the best possible modeling performance for that task. Because these models combine multiple versions of the same underlying model primitive, they are high variance and prone to over-fitting if not tuned properly.**\n",
        "\n",
        "_Tier 1. Models that often win competitions. Fairly robust across datasets and easy to apply in practice:_\n",
        "* Random forests (`scikit.ensemble.RandomForestClassifier/Regressor`): we implemented a simple version of a random forest. The actual one chooses a random subset of input features for each decision tree to increase diversity across trees.\n",
        "* AdaBoost (`scikit.ensemble.AdaBoostClassifier/Regressor`): Remember, this boosting algorithm reweights training examples that previous models struggled with when training new models.\n",
        "* Gradient Boosting (`scikit.ensemble.GradientBoostingClassifier/Regressor`): Recall that gradient boosting trains successive models to reduce the residual error made by previous models.\n",
        "* XGboost (https://xgboost.readthedocs.io/en/stable): This is a popular gradient boosting library on top of decision trees. It is highly optimized and known to have good performance. Unfortunately, it is not in scikit-learn but it is fairly easy to use. To install this, try `!pip install xgboost`. Read its documentation to see its syntax.\n",
        "\n",
        "_Tier 2. Advanced models worth trying but slightly less popular/robust in practice compared to the above:_\n",
        "* Voting (`scikit.ensemble.VotingClassifier/Regressor`): This implementation lets to pick between using the majority vote to make a prediction vs using average predicted probabilities (ie.e., a soft voting strategy).\n",
        "* Combining strategies (`scikit.ensemble.StackingClassifier/Regressor`): This is a generic data structure to combine ensembling techniques together. Just remember to be careful of overfitting if you use this class.\n",
        "* Extremely Randomized Trees (`scikit.ensemble.ExtraTreesClassifier`): This is a close variant of random forests. Like random forests, each decision tree is trained on a random subset of features. But unlike random forests, each decision tree is not trained to split the training data perfectly; rather some randomness is added in to increase diversity.\n",
        "\n",
        "* Deep neural networks: Later courses will cover deep neural networks, which are often SOTA when large training sets are available, but require special optimization and modeling considerations.\n",
        "\n",
        "Overall, if you're using techniques like the above and applying some hyper-parameter tuning to achieve good regularization settings, you will likely produce near SOTA models on most tasks. There is a dependence on datasets and the input/output function being modeled, but these techniques perform well in many production ML systems."
      ],
      "metadata": {
        "id": "PzaSS4S7veeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries"
      ],
      "metadata": {
        "id": "QPzszpkftVj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "\n",
        "We first setup required libraries. Many of these may already be installed by default in Colab."
      ],
      "metadata": {
        "id": "wi6JJI13znVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install xgboost\n",
        "!pip install librosa"
      ],
      "metadata": {
        "id": "zYOhybWNz-89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1a479b-9023-49c6-b948-9aebdec7d738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.10.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (24.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb  # gradient boosting library\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "kHjGXhH1B8Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo Dataset"
      ],
      "metadata": {
        "id": "6FvZ16mhFfAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To showcase these model techniques, we will use a popular toy dataset containing patient features of a digitized image of a fine needle aspirate of a breast mass cell nuclei. The task is to predict if the cell is benign or malignant. This dataset is publicly available on scikit-learn."
      ],
      "metadata": {
        "id": "s_vj-CZrChGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = datasets.load_breast_cancer()\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "y1XlLY7eCwaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b7f1a8-31ce-4352-d848-adaedeef4df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(455, 30) (455,)\n",
            "(114, 30) (114,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling: Combining Model Diversity"
      ],
      "metadata": {
        "id": "4HMZscmetekW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised learning algorithms try to fit a model that performs well on a prediction problem. However, for complex prediction problems, there may be multiple solutions. For example, we use [ensembling](https://en.wikipedia.org/wiki/Ensemble_learning) in the form of random forest classifiers. Ensembling takes a collection of diverse models that solve the same prediction problem into a single model with potentially better performance.\n",
        "\n",
        "**Deeper into the random forest**\n",
        "\n",
        "In Week 1, we played with a decision tree, which splits the feature space into partitions that are mapped to target labels. However, the partitioning is a somewhat random process, and fitting the decision tree with different random seeds can result in very different partitions.\n",
        "\n",
        "A random forest is an ensemble of decision trees. It trains multiple decision trees independently. Then, to make a prediction on a new example, each decision tree makes a prediction as a \"vote\", and the forest spits out the the majority vote. For example, if the ensemble has 10 trees and 8 of them predict a positive label whereas 2 of them predict a negative label, the ensemble will predict positive (since 8 > 2).\n",
        "\n",
        "There many ways to \"aggregate\" votes, with a majority vote being a common and simple approach. Alternative approaches include averaging (for regression problems) or more complex statistical methods like [Bayesian optimality](https://machinelearningmastery.com/bayes-optimal-classifier). Some of these options are implemented as part of the scikit [classifier parameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
        "\n",
        "The benefit of the random forest (and ensembling in general) is that individual errors are averaged out. Each single decision tree might do poorly on some examples, but across a large collection of trees, the majority of trees might perform well on all examples. The downside to ensembles is cost! If we want to ensemble 100 models, we still need to make predictions with 100 models.\n",
        "\n",
        "Other versions of ensembling combine different model families. Nothing prevents you from ensembling a decision tree with a linear model. In fact, this increased diversity may be advantageous."
      ],
      "metadata": {
        "id": "bbZ-pgoM9JSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Ensemble` class trains a collection of models of a given type, and then makes predictions using majority vote aggregation. We train a single decision tree as well as an ensemble of decision trees on the cancer dataset we acquired from Kaggle. **You do not need to use this class directly to build tree ensembles on a practical problem. The `Ensemble` class handles creating ensembles in general. There is a pre-built ensembled tree classifier available via `sklearn.ensemble.RandomForestClassifier`**"
      ],
      "metadata": {
        "id": "x9Jal8dFzKUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ensemble:\n",
        "  \"\"\"\n",
        "  Example implementation of an ensemble with majority vote aggregation.\n",
        "\n",
        "  @models: a list of scikit-learn models.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, models):\n",
        "    self._models = models\n",
        "    self._size = len(models)\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    for i in range(self._size):\n",
        "      self._models[i].fit(X_train, y_train)\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    preds = []\n",
        "    for i in range(self._size):\n",
        "      pred_i = self._models[i].predict(X_test)\n",
        "      preds.append(pred_i)\n",
        "    preds = np.vstack(preds)\n",
        "    agg = []\n",
        "    for j in range(preds.shape[1]):\n",
        "      agg.append(np.bincount(preds[:, j]).argmax())\n",
        "    agg = np.array(agg)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "lEluT3249IgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply it to the cancer dataset\n",
        "\n",
        "model = DecisionTreeClassifier(max_leaf_nodes=20)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(f'Single test acc: {acc}')\n",
        "\n",
        "models = [DecisionTreeClassifier(max_leaf_nodes=20) for _ in range(30)]\n",
        "ensemble = Ensemble(models)\n",
        "ensemble.fit(X_train, y_train)\n",
        "y_pred = ensemble.predict(X_test)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(f'Ensemble test acc: {acc}')"
      ],
      "metadata": {
        "id": "NPgbJXPMEEPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074a91a8-0310-4c88-9036-f70265054ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single test acc: 0.9210526315789473\n",
            "Ensemble test acc: 0.9385964912280702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the ensemble does a bit better than a single decision tree!"
      ],
      "metadata": {
        "id": "WgLYfnraF3oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging: Combining Data Subsets"
      ],
      "metadata": {
        "id": "IDvA7pEytxvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a training dataset $D$ with $N$ entries. We then generate $M$ new datasets $D_1, \\ldots D_M$, each with size $N'$. We do this by sampling entries from $D$ uniformly with replacement (so that duplicated examples are possible). We then train $M$ models on each of the $M$ new datasets. For a new example, we again make $M$ predictions using each of the $M$ models and similarly aggregate the predictions by averaging or majority voting.\n",
        "\n",
        "This technique is quite similar to ensembling, except rather than relying on model diversity, we rely on data diversity. The hypothesis is similar, that by composing together models that have individual biases from different subsets of data, we can make better predictions in aggregate. Like ensembling, bagging is quite expensive in computation and storage, as now we must store many copies of the dataset as well as a model per copy."
      ],
      "metadata": {
        "id": "jzkOB_gD6pQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we show a `Bagging` class that trains models of a given type on bootstrapped samples from the training dataset.  It then makes predictions using majority vote again. Again, we train a single decision tree, and then use bagging to train and combine a collection of decision trees:"
      ],
      "metadata": {
        "id": "tAdKTc9q0L15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny8CtudqxoDm"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class Bagging:\n",
        "  \"\"\"\n",
        "  Example implementation of bagging with majority vote aggregation.\n",
        "\n",
        "  @model: a scikit-learn model.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model, M, N_prime):\n",
        "    self._model = model\n",
        "    self._models = None\n",
        "    self._M = M\n",
        "    self._N_prime = N_prime\n",
        "    self._rs = np.random.RandomState(42)\n",
        "\n",
        "  def bootstrap(self, X, y):\n",
        "    subsets = []\n",
        "    N = len(X)\n",
        "    for i in range(self._M):\n",
        "      indices = self._rs.choice(N, self._N_prime, replace=True)\n",
        "      X_i = X[indices]\n",
        "      y_i = y[indices]\n",
        "      subsets.append((X_i, y_i))\n",
        "    return subsets\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    subsets = self.bootstrap(X_train, y_train)\n",
        "    models = []\n",
        "    for i in range(self._M):\n",
        "      X_i, y_i = subsets[i]\n",
        "      model_i = copy.deepcopy(self._model)  # Q: why copy?\n",
        "      model_i.fit(X_i, y_i)\n",
        "      models.append(model_i)\n",
        "    self._models = models\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    preds = []\n",
        "    for i in range(self._M):\n",
        "      pred_i = self._models[i].predict(X_test)\n",
        "      preds.append(pred_i)\n",
        "    preds = np.vstack(preds)\n",
        "    agg = []\n",
        "    for j in range(preds.shape[1]):\n",
        "      agg.append(np.bincount(preds[:, j]).argmax())\n",
        "    agg = np.array(agg)\n",
        "    return agg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply it to the cancer dataset\n",
        "\n",
        "model = DecisionTreeClassifier(max_leaf_nodes=20)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(f'Single test acc: {acc}')\n",
        "\n",
        "model = DecisionTreeClassifier(max_leaf_nodes=20)\n",
        "ensemble = Bagging(model, 30, 100)\n",
        "ensemble.fit(X_train, y_train)\n",
        "y_pred = ensemble.predict(X_test)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(f'Bagging test acc: {acc}')"
      ],
      "metadata": {
        "id": "dLLQ5fGJIXYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df16b065-4ad1-4ed9-ff3d-1ebd8a0b30bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single test acc: 0.9473684210526315\n",
            "Bagging test acc: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting: Combining Model Error"
      ],
      "metadata": {
        "id": "LvW3zGmFt0_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have seen methods that use independent copies of the model. Boosting presents an alternative way to combine models. First, train your first model which correctly classifies some examples and incorrectly classifies some other examples. Then, train a second model to \"correct\" the examples that the first model got wrong. Then, train a third model to \"correct\" the second model, and so on.\n",
        "\n",
        "The \"correction\" process can be realized in many different ways.\n",
        "- One popular method, called AdaBoost, upweights examples that the $T-1$-th model got wrong when training the $T$-th model.\n",
        "- Another popular framework, called Gradient Boosting, trains the $T$-th model to predict the residual (aka difference) between the $T-1$-th model and the true label.\n",
        "\n",
        "In either case, the final prediction is made by summing the predictions overall $T$ models. It is possible to have boosting frameworks spit out the $T$-th model as the most powerful one, rather than summing over all $T$."
      ],
      "metadata": {
        "id": "Gy-vIS-g60i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It's easier to present Gradient Boosting in the regression setting.\n",
        "# Let's use a different dataset from scikit-learn. This one contains\n",
        "# patient features where the label is if the patient has diabetes or not.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "dataset2 = datasets.load_diabetes()\n",
        "X2 = dataset2.data\n",
        "y2 = dataset2.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "y2 = scaler.fit_transform(y2[:, np.newaxis]).ravel()\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X2, y2, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "print(X2_train.shape, y2_train.shape)\n",
        "print(X2_test.shape, y2_test.shape)"
      ],
      "metadata": {
        "id": "R5uo7wWUOzXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44d3fc8-0019-44bf-c11b-b6443667051a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(353, 10) (353,)\n",
            "(89, 10) (89,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "class GradientBoosting:\n",
        "  \"\"\"\n",
        "  Example implementation of gradient boosting.\n",
        "\n",
        "  @model: a scikit-learn model.\n",
        "  \"\"\"\n",
        "  def __init__(self, model, M):\n",
        "    self._model = model\n",
        "    self._models = None\n",
        "    self._M = M\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    r_train = np.zeros_like(y_train)  # residuals\n",
        "    models = []\n",
        "\n",
        "    for i in range(self._M):\n",
        "      model_i = copy.deepcopy(self._model)\n",
        "\n",
        "      if i == 0:\n",
        "        model_i.fit(X_train, y_train)\n",
        "      else:\n",
        "        model_i.fit(X_train, r_train)\n",
        "\n",
        "      models.append(model_i)\n",
        "\n",
        "      pred_i = model_i.predict(X_train)\n",
        "\n",
        "      if i == 0:\n",
        "        r_train = y_train - pred_i\n",
        "      else:\n",
        "        r_train = r_train - pred_i\n",
        "\n",
        "    self._models = models\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    preds = np.zeros(len(X_test))\n",
        "    for i in range(self._M):\n",
        "      pred_i = self._models[i].predict(X_test)\n",
        "      preds += pred_i\n",
        "    return preds"
      ],
      "metadata": {
        "id": "ZbynX6ak61nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply it to the diabetes dataset\n",
        "\n",
        "model = DecisionTreeRegressor(max_leaf_nodes=20)\n",
        "model.fit(X2_train, y2_train)\n",
        "y2_pred = model.predict(X2_train)\n",
        "mse = np.mean((y2_pred - y2_train)**2)\n",
        "print(f'Single train mse: {mse}')\n",
        "y2_pred = model.predict(X2_test)\n",
        "mse = np.mean((y2_pred - y2_test)**2)\n",
        "print(f'Single test mse: {mse}')\n",
        "\n",
        "model = DecisionTreeRegressor(max_leaf_nodes=20)\n",
        "ensemble = GradientBoosting(model, 10)\n",
        "ensemble.fit(X2_train, y2_train)\n",
        "y2_pred = ensemble.predict(X2_train)\n",
        "mse = np.mean((y2_pred - y2_train)**2)\n",
        "print(f'Boosting train mse: {mse}')\n",
        "y2_pred = ensemble.predict(X2_test)\n",
        "mse = np.mean((y2_pred - y2_test)**2)\n",
        "print(f'Boosting test mse: {mse}')"
      ],
      "metadata": {
        "id": "qks6xiekQYGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7869c1c7-d2f5-4e9f-e5fc-03f2405b6bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single train mse: 0.3417244879354889\n",
            "Single test mse: 0.5824366346672816\n",
            "Boosting train mse: 0.015136124358794354\n",
            "Boosting test mse: 0.9836974828675755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you notice above, gradient boosting could be over-susceptible to overfitting as it is a very expressive model. Boosting tends to work much better on larger datasets."
      ],
      "metadata": {
        "id": "PKSZFRNDTAaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosting can also be used in classification although computing residuals is slightly more technical. See this [link](https://towardsdatascience.com/gradient-boosting-classification-explained-through-python-60cc980eeb3d) for more details. Rather than implement it, we can use the existing library in scikit-learn."
      ],
      "metadata": {
        "id": "ZAonmxUdTNG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Back to breast cancer dataset\n",
        "\n",
        "model = DecisionTreeClassifier(max_leaf_nodes=20)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(f'Single test acc: {acc}')\n",
        "\n",
        "ensemble = GradientBoostingClassifier(n_estimators=30, max_leaf_nodes=20)\n",
        "ensemble.fit(X_train, y_train)\n",
        "y_pred = ensemble.predict(X_test)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(f'Boosting test acc: {acc}')"
      ],
      "metadata": {
        "id": "YC3Z82qnTMmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efab2b42-87cf-4cb9-c3a6-0b70ff5fc58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single test acc: 0.9473684210526315\n",
            "Boosting test acc: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a last note, there is a very popular library for gradient boosting decision trees called `xgboost`. You may find this library helpful in your own experiments. XGBoost is regarded as a default tool for producing SOTA results on many small to medium ML classification tasks. The library is well optimized and produces quality models consistently. As datasets grow, deep learning techniques become more relevant for achieving top performance."
      ],
      "metadata": {
        "id": "N25n7olnTx0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xgboost requires its own data structures\n",
        "d_train = xgb.DMatrix(X_train, label=y_train)\n",
        "d_test = xgb.DMatrix(X_test)\n",
        "\n",
        "param = {\n",
        "    'n_estimators': 30,\n",
        "    'max_depth': 5,  # the maximum depth of each tree\n",
        "    'eta': 0.3,  # the training step for each iteration\n",
        "    'silent': 1,  # logging mode - quiet\n",
        "    'objective': 'binary:logistic',  # error evaluation for multiclass training\n",
        "}\n",
        "\n",
        "num_round = 20  # the number of training iterations\n",
        "bst = xgb.train(param, d_train, num_round)\n",
        "y_pred = bst.predict(d_test)\n",
        "y_pred = np.round(y_pred).astype(int)"
      ],
      "metadata": {
        "id": "xqF81YcpT7VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854dd042-45e7-45cb-cd0f-f7a49dd11c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:08:20] WARNING: /workspace/src/learner.cc:742: \n",
            "Parameters: { \"n_estimators\", \"silent\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = np.mean(y_pred == y_test)\n",
        "print(f'xgboost test acc: {acc}')"
      ],
      "metadata": {
        "id": "ZRVnj6sGUeI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132df6d6-4bf1-4ed4-eba8-716ba22fde57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost test acc: 0.956140350877193\n"
          ]
        }
      ]
    }
  ]
}